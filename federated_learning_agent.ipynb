{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madch3m/Federated-learning-ml-graph/blob/experimental_rd/federated_learning_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv6hav8YTmSU",
        "outputId": "1c89050c-2a5c-4e9e-df47-53a5d0e530e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-utils\n",
            "  Downloading torch-utils-0.1.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torch-utils) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch-utils) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torch-utils) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torch-utils) (3.0.3)\n",
            "Building wheels for collected packages: torch-utils\n",
            "  Building wheel for torch-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-utils: filename=torch_utils-0.1.2-py3-none-any.whl size=6188 sha256=ccdfa78ee7dc8d0e252151f2a0d4e0b8003af3977b7a9664552f629dd257d974\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/06/32/1d26da91e30177d171ecb60995273ad8709ca2b6ce66ccefa7\n",
            "Successfully built torch-utils\n",
            "Installing collected packages: torch-utils\n",
            "Successfully installed torch-utils-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparams for the federated graph"
      ],
      "metadata": {
        "id": "SrlLpvbJRNBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ycOAkUcSQCe8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "@dataclass\n",
        "class HParams:\n",
        "  num_clients: int = 10\n",
        "  smple_clients: float = 0.5\n",
        "  local_epochs: int = 2\n",
        "  local_batch_size: int = 64\n",
        "  rounds: int = 1\n",
        "  lr: float = 0.01\n",
        "  momentum: float = 0.0\n",
        "  seed: int = 42\n",
        "  iid: bool = True\n",
        "  device: str = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Net"
      ],
      "metadata": {
        "id": "8cxEZwc0SN7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from typing import List, Tuple\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Conv2d(1,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.Conv2d(32,64,3,padding=1), nn.ReLU(),nn.MaxPool2d(2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(64 * 7 * 7, 128), nn.ReLU(),\n",
        "        nn.Linear(128,10)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "      return self.net(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "PacE7ULvSR6H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading and splitting for clients"
      ],
      "metadata": {
        "id": "o5inAG1-TZNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hp = HParams()\n",
        "random.seed(hp.seed)\n",
        "torch.manual_seed(hp.seed)\n",
        "def load_data() -> Tuple[List[Subset], torch.utils.data.Dataset]:\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    if hp.iid:\n",
        "        sizes = [len(train) // hp.num_clients] * hp.num_clients\n",
        "\n",
        "        sizes[-1] += len(train) - sum(sizes)\n",
        "        shards = random_split(train, sizes, generator=torch.Generator().manual_seed(hp.seed))\n",
        "        clients = [Subset(train, s.indices) for s in shards]\n",
        "        return clients, test\n",
        "\n",
        "    else:\n",
        "        targets = torch.tensor(train.targets)\n",
        "        sorted_idx = targets.sort()[1].tolist()\n",
        "        sorted_ds = Subset(train, sorted_idx)\n",
        "        sizes = [len(sorted_ds) // hp.num_clients] * hp.num_clients\n",
        "        sizes[-1] += len(sorted_ds) - sum(sizes)\n",
        "        shards = []\n",
        "        start = 0\n",
        "        for size in sizes:\n",
        "            idxs = list(range(start, start + size))\n",
        "            shards.append(Subset(sorted_ds, idxs))\n",
        "            start += size\n",
        "        return shards, test\n",
        "\n"
      ],
      "metadata": {
        "id": "hEqyfXP_TdvV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Client Logic"
      ],
      "metadata": {
        "id": "XyDzpLkcWp4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def client_update(global_model: nn.Module, dataset: Subset) -> Tuple[Dict[str, torch.Tensor], int]:\n",
        "    model = deepcopy(global_model).to(hp.device)\n",
        "    model.train()\n",
        "    loader = DataLoader(dataset, batch_size=hp.local_batch_size, shuffle=True, drop_last=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=hp.lr, momentum=hp.momentum)\n",
        "\n",
        "    for _ in range(hp.local_epochs):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(hp.device), y.to(hp.device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(x), y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return deepcopy(model.state_dict()), len(dataset)"
      ],
      "metadata": {
        "id": "xjkXtryHT8vz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregation Logic"
      ],
      "metadata": {
        "id": "seGWmIYIYR3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def fedavg(global_model: nn.Module, client_states: List[Tuple[Dict[str,torch.Tensor],int]]):\n",
        "    total_samples = sum(n for _, n in client_states)\n",
        "\n",
        "    avg_state = {k: torch.zeros_like(v, device=hp.device) for k, v in global_model.state_dict().items()}\n",
        "\n",
        "    for state_dict, n in client_states:\n",
        "        weight = n / total_samples\n",
        "        for k in avg_state.keys():\n",
        "          avg_state[k] += state_dict[k].to(hp.device) * weight\n",
        "    global_model.load_state_dict(avg_state)"
      ],
      "metadata": {
        "id": "VkD8zQ0tee2t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, testset) -> Tuple[float,float]:\n",
        "    model.eval().to(hp.device)\n",
        "    loader = DataLoader(testset, batch_size=512, shuffle=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    correct, total, total_loss = 0, 0, 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(hp.device), y.to(hp.device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return correct / total, total_loss / total\n"
      ],
      "metadata": {
        "id": "C6sErAVtYTrM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orchestrator"
      ],
      "metadata": {
        "id": "dKQ9XmpwZcMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import TIMEOUT_MAX\n",
        "import matplotlib.pyplot as plt\n",
        "import time, random, torch\n",
        "\n",
        "def orchestrate():\n",
        "    clients, testset = load_data()\n",
        "    global_model = CNN().to(hp.device)\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    rounds_run = 0\n",
        "    best_acc = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    acc_history = []   # for plotting accuracy\n",
        "    loss_history = []  # for plotting loss\n",
        "    rounds_history = []   # for plotting rounds on the x-axis\n",
        "\n",
        "    TIMEOUT_MAX = 6 * 60 # 6-minute timeout\n",
        "    TARGET_ACC = 0.90\n",
        "\n",
        "    print(\"Starting federated training...\\n\")\n",
        "\n",
        "    while True:\n",
        "        elapsed = time.perf_counter() - start_time\n",
        "        if elapsed >= TIMEOUT_MAX:\n",
        "            print(\"Timeout hit. Stopping.\")\n",
        "            break\n",
        "\n",
        "        rounds_run += 1\n",
        "        m = max(1, int(hp.smple_clients * hp.num_clients))\n",
        "        selected = random.sample(range(hp.num_clients), m)\n",
        "\n",
        "        client_states = []\n",
        "        for cid in selected:\n",
        "            state, n_samples = client_update(global_model, clients[cid])\n",
        "            client_states.append((state, n_samples))\n",
        "\n",
        "        fedavg(global_model, client_states)\n",
        "\n",
        "        acc, los = evaluate(global_model, testset)\n",
        "\n",
        "        acc_history.append(acc)\n",
        "        loss_history.append(los)\n",
        "        rounds_history.append(rounds_run)\n",
        "\n",
        "        print(f\"Round {rounds_run:02d} | acc={acc*100:.2f}% | loss={los:.4f} | target_acc={TARGET_ACC*100}% | timeout={TIMEOUT_MAX} | elapsed={time.perf_counter() - start_time}\")\n",
        "        if acc >= TARGET_ACC:\n",
        "            print(f\"Target accuracy reached ({acc*100:.2f}%) at round {rounds_run}.\")\n",
        "            break\n",
        "\n",
        "    torch.save(global_model.state_dict(), \"mnist_cnn.pt\")\n",
        "    print(\"Model saved\")\n",
        "\n",
        "    # Plot metrics\n",
        "    plt.figure(figsize=(10,4)) # (width, height) of the plot\n",
        "    plt.subplot(1,2,1) # (number of rows, number of columns, plot index)\n",
        "    plt.plot(rounds_history, [a*100 for a in acc_history], 'o-', label='Accuracy (%)')\n",
        "    plt.xlabel('Communication Rounds')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Accuracy vs Communication Rounds')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds_history, loss_history, 'o-', color='orange', label='Loss')\n",
        "    plt.xlabel('Communication Rounds')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss vs Communication Rounds')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # plt.savefig(\"federated_training_progress.png\")\n"
      ],
      "metadata": {
        "id": "UiructFRZdwc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orchestrate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxP3Z6emRHFU",
        "outputId": "0cd7af86-fd0b-4a86-98ab-bb7ffbe8cdc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting federated training...\n",
            "\n",
            "Round 01 | acc=59.22% | loss=2.0545 | target_acc=90.0% | timeout=360 | elapsed=99.66333791099987\n"
          ]
        }
      ]
    }
  ]
}